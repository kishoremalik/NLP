{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MDvi-FhO7Sjo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Text Classification</h1>\n",
        "There are three main types of classification:\n",
        "\n",
        ">> 1) Binary: Two mutually exclusive categories (e.g., Spam detection)\n",
        "\n",
        ">> 2)Multiclass: More than 2 mutually exclusive categories (e.g., Language detection)\n",
        "\n",
        ">> 3) Multilabel: Non-mutually exclusive categories (e.g., movie genres)"
      ]
    },
    {
      "metadata": {
        "id": "Mw1lzrUI75hR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Binary text classification problem</h1> \n",
        ">> We will address the binary problem of detecting Sport related documents vs any other type of documents. In order to do this we will create an artificial (and very small collection).\n",
        "\n",
        ">>1) Define a set of labelled documents that will be our training dataset. These are the documents the classifier will learn from in order to categorise future unseen documents\n",
        "\n",
        ">> 2) Define a set of labelled documents that will be our testing dataset. These will be the \"unseen\" documents that the classifier will predict (without having being trained with them)\n",
        "\n",
        ">> 3) Represent our training and testing documents\n",
        "\n",
        ">> 4) Train the classifier based on the training data\n",
        "\n",
        ">> 5) Predict the labels for the testing documents"
      ]
    },
    {
      "metadata": {
        "id": "UlsT8eEJ74vz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aHEfTzXk72hc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train and test data. Both the full documents and their labels (\"Sports\" vs \"Non Sports\")\n",
        "train_data = ['Football: a great sport', 'The referee has been very bad this season', 'Our team scored 5 goals', 'I love tennis',\n",
        "              'Politics is in decline in the UK', 'Brexit means Brexit', 'The parlament wants to create new legislation',\n",
        "              'I so want to travel the world']\n",
        "train_labels = [\"Sports\",\"Sports\",\"Sports\",\"Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\"]\n",
        "\n",
        "test_data = ['Swimming is a great sport', \n",
        "             'A lot of policy changes will happen after Brexit', \n",
        "             'The table tennis team will travel to the UK soon for the European Championship']\n",
        "test_labels = [\"Sports\",\"Non Sports\",\"Sports\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmVrd2KYAmiT",
        "colab_type": "code",
        "outputId": "90f07a67-9b53-48f6-b5f6-66e223deb6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('feature {},label {}'.format(train_data[1],train_labels[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature The referee has been very bad this season,label Sports\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bWVTn3mV9CLq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Representation of the data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "vectorised_test_data = vectorizer.transform(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T0XJKqJK9QlT",
        "colab_type": "code",
        "outputId": "25d4a44a-562d-4345-93a6-592b4bee93d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "vectorised_train_data[1].toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3675562 , 0.3675562 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.3675562 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.3675562 , 0.        ,\n",
              "        0.3675562 , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.23306022, 0.3675562 , 0.        , 0.        , 0.        ,\n",
              "        0.3675562 , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "aq_GZElJ9wus",
        "colab_type": "code",
        "outputId": "62145a30-fe4a-4f08-ba46-76f57309dc09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the classifier given the training data\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(vectorised_train_data, train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "     verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "_AxAM55y93HB",
        "colab_type": "code",
        "outputId": "f104f72f-fae0-49c0-ef5c-c49b7a7a5cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Predict the labels for the test documents (not used for training)\n",
        "print(classifier.predict(vectorised_test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sports' 'Non Sports' 'Non Sports']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bVC5EJ9SHGKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Navie Bay</h1>"
      ]
    },
    {
      "metadata": {
        "id": "7358sjYyHE7f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNpp50k2HbX9",
        "colab_type": "code",
        "outputId": "e3f547fc-9b3a-43ca-e441-4d8a54ad493f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model = GaussianNB()\n",
        "\n",
        "model.fit(vectorised_train_data.toarray(), train_labels)\n",
        "\n",
        "#Predict Output \n",
        "predicted= model.predict(vectorised_test_data.toarray())\n",
        "print (predicted)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sports' 'Non Sports' 'Non Sports']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hHBNIuREIY2y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Lets try again, with stop-word removal this time</h1>\n",
        "\n",
        ">>  # it is observed the remova of STOP words give good accuracy"
      ]
    },
    {
      "metadata": {
        "id": "Y5APqTREKhMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def text_processing_weithoutSTOPWORD():\n",
        "  stop_words = stopwords.words(\"english\")\n",
        "  vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "  vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "  vectorised_test_data = vectorizer.transform(test_data)\n",
        "  classifier = LinearSVC()\n",
        "  classifier.fit(vectorised_train_data, train_labels)\n",
        "  print(classifier.predict(vectorised_test_data))\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-p5JleGTLAs8",
        "colab_type": "code",
        "outputId": "9f9e0446-5a7a-4437-b5ec-55c78ac0c28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text_processing_weithoutSTOPWORD()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sports' 'Non Sports' 'Sports']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BwQt9w2mMGyq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> multiclass classification</h1>\n",
        "\n",
        ">> We will address the multi-class problem of detecting the language of a sentence based on 3 mutually exclusive languages (e.g., Spanish, English and French). For the sake of this example, we assume those are the only 3 languages that the documents can have. As before, we will create an artificial (and very small collection) with similar steps"
      ]
    },
    {
      "metadata": {
        "id": "OO0Y5sAnMcfH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def multiClass_prediction(train_data,train_labels,test_data):\n",
        "  vectorizer = TfidfVectorizer() # Note, we are not doing stop-word removal. Stop words could be beneficial in this problems\n",
        "  vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "  vectorised_test_data = vectorizer.transform(test_data)\n",
        "  \n",
        "  classifier = LinearSVC()\n",
        "  classifier.fit(vectorised_train_data, train_labels)\n",
        "  predictions = classifier.predict(vectorised_test_data)\n",
        "  print(predictions)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwAoTiGPNFlC",
        "colab_type": "code",
        "outputId": "d1b34f48-e223-45d2-cfb6-b63341717f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data = ['PyCon es una gran conferencia', 'Aprendizaje automatico esta listo para dominar el mundo dentro de poco',\n",
        "             'This is a great conference with a lot of amazing talks', 'AI will dominate the world in the near future',\n",
        "             'Dix chiffres por resumer le feuilleton de la loi travail']\n",
        "train_labels = [\"SP\", \"SP\", \"EN\", \"EN\", \"FR\"]\n",
        "\n",
        "test_data = ['Estoy preparandome para dominar las olimpiadas', 'Me gustaria mucho aprender el lenguage de programacion Scala',\n",
        "             'Machine Learning is amazing','Hola a todos']\n",
        "test_labels = [\"SP\", \"SP\", \"EN\", \"SP\"]\n",
        "\n",
        "multiClass_prediction(train_data,train_labels,test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SP' 'SP' 'EN' 'EN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i867w5HwOsPe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">> # the previous prediction is not correct (expected \"SP\", prediction is \"EN\") \n"
      ]
    },
    {
      "metadata": {
        "id": "y-IUymOMPIer",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## below code for good Accuracy of Language modeling "
      ]
    },
    {
      "metadata": {
        "id": "a-aw8ppvPB55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bWsOxEz2PYBy",
        "colab_type": "code",
        "outputId": "001240bf-940c-4247-9a4c-3701a4c4ad6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Artificial (and small) dataset. Sports and Politics\n",
        "\n",
        "train_data = ['Football: a great sport', 'The referee has been very bad this season', 'Our team scored 5 goals', 'I love tennis',\n",
        "              'Politics is in decline in the UK', 'Brexit means Brexit', 'The parlament wants to create new legislation',\n",
        "              'I so want to travel the world', \n",
        "              'The goverment will increase the budget for sports in the UK after the victories in the Olimpic Games',\n",
        "              \"O'Reilly has a great conference this year\"]\n",
        "\n",
        "train_labels = [[\"Sports\"], [\"Sports\"], [\"Sports\"], [\"Sports\"],[\"Politics\"],[\"Politics\"],[\"Politics\"],[],[\"Politics\", \"Sports\"],[]]\n",
        "\n",
        "test_data = ['Swimming is a great sport', \n",
        "             'A lot of policy changes will happen after Brexit', \n",
        "             'The table tennis team will travel to the UK soon for the European Championship',\n",
        "             'The goverment will increase the budget for sports in the UK after the victories in the Olimpic Games',\n",
        "             'PyCon is my favourite conference'] \n",
        "\n",
        "test_labels = [[\"Sports\"], [\"Politics\"], [\"Sports\"], [\"Politics\",\"Sports\"],[]] \n",
        "\n",
        "# Change the representation of our data as a list of bit lists \n",
        "mlb = MultiLabelBinarizer()\n",
        "binary_train_labels = mlb.fit_transform(train_labels)\n",
        "binary_test_labels = mlb.transform(test_labels)\n",
        "\n",
        "print(binary_train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b0zMSao2QFT6",
        "colab_type": "code",
        "outputId": "4264f0c8-863b-47bc-8fde-77f5accca6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# Represent \n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
        "vectorised_test_data = vectorizer.transform(test_data)\n",
        "\n",
        "# One classifer built per category using a one vs the rest approach\n",
        "classifier = OneVsRestClassifier(LinearSVC())\n",
        "classifier.fit(vectorised_train_data, binary_train_labels)\n",
        "\n",
        "#Predict\n",
        "predictions = classifier.predict(vectorised_test_data)\n",
        "\n",
        "print(predictions)\n",
        "print()\n",
        "\n",
        "print(mlb.inverse_transform(predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 1]\n",
            " [0 0]]\n",
            "\n",
            "[('Sports',), ('Politics',), ('Sports',), ('Politics', 'Sports'), ()]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}